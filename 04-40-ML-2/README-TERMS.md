## Некоторые термины и определения модуля ML-2 (Обучение с учителем: регрессия) ##

В категории **обучения с учителем** модели можно условно разделить на следующие
основные типы:

- **Линейные модели**: линейная регрессия (для задачи регрессии) и логистическая
регрессия (для задачи классификации) и производные от них.
- **&laquo;Древесные&raquo; модели**: дерево решений и производные от него.
- **Метрические алгоритмы**: метод ближайших соседей и производные от него.
- **Байесовские методы**: метод наивного Байеса и производные от него.
- **Ансамблевые методы**: композиции из методов (бэггинг, стекинг, бустинг).

----

**Регрессия**&nbsp;&mdash; класс задач обучения с учителем, когда по
определённому набору признаков объекта необходимо предсказать числовую целевую
переменную.

**Линейная регрессия (Linear Regression)**&nbsp;&mdash; одна из простейших
моделей для решения задачи регрессии. Главная гипотеза состоит в том, что
рассматриваемая зависимость является линейной.

Общий вид модели в случае, когда целевая переменная зависит от $m$ факторов,
будет иметь следующий вид:
$$\widehat{y}=w_0+\displaystyle\sum_{i=1}^mw_ix_i$$
в однофакторном случае всё просто (обычное уравнение прямой):
$\widehat{y}=w_0+w_1x$ ($w_i$&nbsp;&mdash; **параметры линейной регрессии**).

----

**Метрика**&nbsp;&mdash; численное выражение качества моделирования.

Самые распространённые метрики:    
**MAE (Mean Absolute Error)&nbsp;&mdash; `sklearn.metrics.mean_absolute_error()`**
$$MAE=\frac{1}{n}\displaystyle\sum_{i=1}^n|y_i-\widehat{y_i}|$$
**MAPE (Mean Absolute Percent Error)&nbsp;&mdash; `sklearn.metrics.mean_absolute_percentage_error() * 100`**
$$MAPE=\frac{100\%}{n}\displaystyle\sum_{i=1}^n\frac{|y_i-\widehat{y_i}|}{|y_i|}$$
**MSE (Mean Squared Error)&nbsp;&mdash; `sklearn.metrics.mean_squared_error()`**
$$MSE=\frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\widehat{y_i})^2$$
**RMSE (Root Mean Squared Error)**
$$RMSE=\sqrt{MSE}=\sqrt{\frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\widehat{y_i})^2}$$
**Коэффициент детерминации ($R^2$)&nbsp;&mdash; `sklearn.metrics.r2_score()`**    
$R^2$ описывает, какую долю информации о зависимости (дисперсии) смогла уловить
модель. Удовлетворительным $R^2$ считается показатель выше 0.5: чем ближе к 1,
тем лучше. Отрицательные значения говорят о том, что построенная модель
настолько плоха, что лучше было бы присвоить всем ответам среднее значение.
$$R^2=1-\frac{MSE}{MSE_{mean}}$$
$$MSE_{mean}=\frac{1}{n}\displaystyle\sum_{i=1}^n(y_i-\bar{y_i})^2$$
где:    
$y$&nbsp;&mdash; истинные ответы,    
$\widehat{y}$&nbsp;&mdash; предсказания,    
$\bar{y}$&nbsp;&mdash; среднее по вектору правильных ответов,    
$n$&nbsp;&mdash; количество примеров.

----

**Градиентный спуск (Gradient descent)**&nbsp;&mdash; самый используемый
алгоритм минимизации функции потерь. Он применяется почти в каждой модели
машинного обучения и является наиболее простым в реализации из всех методов
численной оптимизации.

**Стохастический градиентный спуск (Stochastic Gradient Descent, SGD) `sklearn.linear_model.SGDRegressor()`**&nbsp;&mdash;
модификация классического градиентного спуска. В отличие от классического, один
шаг стохастического градиентного спуска производится на основе градиента,
рассчитанного не по всей выборке, а только по случайно выбранной части.

----

**Смещение (bias)**&nbsp;&mdash; математическое ожидание разности между истинным
ответом и ответом, выданным моделью. То есть это ожидаемая ошибка модели.
$$bias(\widehat{y})=M[(y-\widehat{y})]$$
В английской литературе:
$$bias(\widehat{y})=E[(y-\widehat{y})]$$
Чем больше смещение, тем слабее модель. Если модель слабая, она не в состоянии
выучить закономерность. Таким образом, налицо недообучение (**underfitting**)
модели.

**Разброс (variance)**&nbsp;&mdash; вариативность ошибки, то, насколько ошибка
будет отличаться, если обучать модель на разных наборах данных. Математически
это дисперсия (разброс) ответов модели.
$$variance(\widehat{y})=D[(y-\widehat{y})]$$
В английской литературе:
$$variance(\widehat{y})=Var[(y-\widehat{y})]$$
Чем больше разброс, тем больше ошибка будет колебаться на разных наборах данных.
Наличие высокого разброса и есть свидетельство переобучения (**overfitting**):
модель подстроилась под конкретный набор данных и даёт высокий разброс ответов
на разных данных.

----

**Полиномиальная регрессия (Polynomial Regression)**&nbsp;&mdash; более сложная
модель, чем линейная регрессия. Вместо уравнения прямой используется уравнение
полинома (многочлена). Степень полинома может быть сколь угодно большой: чем
больше степень, тем сложнее модель.

Пример полинома второй степени для трёх факторов:
$$\widehat{y}=w_0+w_1x_1+w_2x_2+w_3x_3+$$
$$+w_4x_1^2+w_5x_2^2+w_6x_3^2+$$
$$+w_7x_1x_2+w_8x_1x_3+w_9x_2x_3$$

----

**Регуляризация**&nbsp;&mdash; способ уменьшения переобучения моделей машинного
обучения.

**Штраф**&nbsp;&mdash; дополнительное неотрицательное слагаемое в выражении для
функции потерь, которое специально повышает ошибку. За счёт этого слагаемого
метод оптимизации (OLS или SGD) будет находить не истинный минимум функции
потерь, а псевдоминимум.

**L1-регуляризация (Lasso) `sklearn.linear_model.Lasso()`**&nbsp;&mdash;
добавление к функции потерь суммы модулей коэффициентов, умноженных на
коэффициент регуляризации $\alpha$:
$$L_1(w)=MSE+\alpha\displaystyle\sum_{j=1}^m|w_j|$$

**L2-регуляризация (Ridge), или регуляризация Тихонова `sklearn.linear_model.Ridge()`**&nbsp;&mdash;
добавление к функции потерь суммы квадратов коэффициентов, умноженных на
коэффициент регуляризации $\alpha$:
$$L_2(w)=MSE+\alpha\displaystyle\sum_{j=1}^mw_j^2$$

**Эластичная сетка (Elastic Net) `sklearn.linear_model.ElasticNet()`**&nbsp;&mdash;
комбинация из двух методов регуляризации.

Коэффициенты $\alpha$ отвечают за то, насколько сильное смещение будет вноситься
в модель: чем оно больше, тем сильнее будет штраф за переобучение.

***Примечание***    
В реализации **`sklearn`** для решения задачи оптимизации используется
итеративный алгоритм координатного спуска (аналог градиентного спуска, но не
использующий производную).

Обучение линейной регрессии с большим количеством признаков рекомендуется
производить на стандартизованных (нормализованных) данных.

Стандартизацию (нормализацию) полезнее проводить перед генерацией полиномиальных
признаков, иначе можно потерять масштаб полиномов.

***Примечание***    
Регуляризация присутствует и в модели **`SGDRegressor`**, причём она
используется по умолчанию. В инициализаторе данного класса есть параметр
**`penalty`**, который позволяет управлять методом регуляризации. Параметр может
принимать значения **`'l1'`**, **`'l2'`** и **`'elasticnet'`**. По умолчанию
используется L2-регуляризация (`penalty='l2'`). Коэффициент регуляризации
**`alpha`** по умолчанию равен **0.0001** (относительно слабая регуляризация).
Управляя двумя этими параметрами, можно настраивать тип регуляризации в
SGD-методе и её &laquo;силу&raquo;.

----
