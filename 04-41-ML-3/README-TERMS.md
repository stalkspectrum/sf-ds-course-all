## Некоторые термины и определения модуля ML-3 (Обучение с учителем: классификация) ##

**Задача классификации (classification)**&nbsp;&mdash; задача, в которой
предсказывается класс объекта на основе признаков в наборе данных. То есть
задача сводится к предсказанию целевого признака, который является
категориальным.

**Бинарная классификация**&nbsp;&mdash; когда только два класса.
Остальное&nbsp;&mdash; **мультиклассовая (многоклассовая)** классификация.

**Классификатор (classifier)**&nbsp;&mdash; модель, решающая задачу
классификации.

----

**Логистическая регрессия (Logistic Regression)**&nbsp;&mdash; одна из
простейших моделей для решения задачи классификации. Несмотря на простоту,
модель входит в топ часто используемых алгоритмов классификации в Data Science.

Логистическая функция или сигмоида (sigmoid):
$$\sigma(z)=\frac{1}{1+e^{-z}}$$

То есть из модели линейной регрессии
$$z=\bar{w}\bar{x}=w_0+\displaystyle\sum_{j=1}^nw_jx_j$$
тогда $\widehat{P}$&nbsp;&mdash; вероятность в бинарной классификации, принадлежит ли
данный объект к классу 1:
$$\widehat{P}=\sigma(z)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-\bar{w}\bar{x}}}$$

----

**Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE)**&nbsp;&mdash;
оценка того, насколько вероятно получить истинное значение целевой переменной
$y$ при данных $x$ и параметрах $w$, что позволяет получить
**Функцию правдоподобия**:
$$likelihood=\displaystyle\sum_i^n(y_ilog(\widehat{P_i})+(1-y_i)log(1-\widehat{P_i}))\to max_w$$
где

- $n$&nbsp;&mdash; количество наблюдений
- $y_i$&nbsp;&mdash; это истинный класс (1 или 0) для $i$-ого объекта из набора
данных
- $\widehat{P_i}=\sigma(z_i)$&nbsp;&mdash; предсказанная с помощью логистической
регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора
данных
- $z_i$&nbsp;&mdash; результат подстановки $i$-ого объекта из набора данных в
уравнение разделяющей плоскости $z_i=\bar{w}\bar{x_i}$
- $log$&nbsp;&mdash; логарифм (обычно используется натуральный логарифм по
основанию $e$&nbsp;&mdash; $ln$)

То есть цель метода&nbsp;&mdash; найти такие параметры $w=(w_0, w_1, w_2,..., w_m)$,
в которых наблюдается максимум **функции правдоподобия**. К сожалению, эта
функция ($likelihood$) не имеет интерпретации, то есть нельзя сказать, что
значит её вычисленный результат в контексте правдоподобия.

**Функция логистических потерь (logloss) или кросс-энтропия (cross-entropy loss)**
получается, если поставить перед функцией $likelihood$ минус, и задача
оптимизации меняется на противоположную: был поиск максимума&nbsp;&mdash; теперь
поиск минимума:
$$L(w)=logloss=-\displaystyle\sum_i^n(y_ilog(\widehat{P_i})+(1-y_i)log(1-\widehat{P_i}))\to min_w$$

----
