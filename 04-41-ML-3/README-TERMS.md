## Некоторые термины и определения модуля ML-3 (Обучение с учителем: классификация) ##

**Задача классификации (classification)**&nbsp;&mdash; задача, в которой
предсказывается класс объекта на основе признаков в наборе данных. То есть
задача сводится к предсказанию целевого признака, который является
категориальным.

**Бинарная классификация**&nbsp;&mdash; когда только два класса.
Остальное&nbsp;&mdash; **мультиклассовая (многоклассовая)** классификация.

**Классификатор (classifier)**&nbsp;&mdash; модель, решающая задачу
классификации.

----

**Логистическая регрессия (Logistic Regression)**&nbsp;&mdash; одна из
простейших моделей для решения задачи классификации. Несмотря на простоту,
модель входит в топ часто используемых алгоритмов классификации в Data Science.

Логистическая функция или сигмоида (sigmoid):
$$\sigma(z)=\frac{1}{1+e^{-z}}$$

То есть из модели линейной регрессии
$$z=\bar{w}\bar{x}=w_0+\displaystyle\sum_{j=1}^nw_jx_j$$
тогда $\widehat{P}$&nbsp;&mdash; вероятность в бинарной классификации, принадлежит ли
данный объект к классу 1:
$$\widehat{P}=\sigma(z)=\frac{1}{1+e^{-z}}=\frac{1}{1+e^{-\bar{w}\bar{x}}}$$

----

**Метод максимального правдоподобия (Maximum Likelihood Estimation, MLE)**&nbsp;&mdash;
оценка того, насколько вероятно получить истинное значение целевой переменной
$y$ при данных $x$ и параметрах $w$, что позволяет получить
**Функцию правдоподобия**:
$$likelihood=\displaystyle\sum_i^n(y_ilog(\widehat{P_i})+(1-y_i)log(1-\widehat{P_i}))\to max_w$$
где

- $n$&nbsp;&mdash; количество наблюдений
- $y_i$&nbsp;&mdash; это истинный класс (1 или 0) для $i$-ого объекта из набора
данных
- $\widehat{P_i}=\sigma(z_i)$&nbsp;&mdash; предсказанная с помощью логистической
регрессии вероятность принадлежности к классу 1 для $i$-ого объекта из набора
данных
- $z_i$&nbsp;&mdash; результат подстановки $i$-ого объекта из набора данных в
уравнение разделяющей плоскости $z_i=\bar{w}\bar{x_i}$
- $log$&nbsp;&mdash; логарифм (обычно используется натуральный логарифм по
основанию $e$&nbsp;&mdash; $ln$)

То есть цель метода&nbsp;&mdash; найти такие параметры $w=(w_0, w_1, w_2,..., w_m)$,
в которых наблюдается максимум **функции правдоподобия**. К сожалению, эта
функция ($likelihood$) не имеет интерпретации, то есть нельзя сказать, что
значит её вычисленный результат в контексте правдоподобия.

**Функция логистических потерь (logloss) или кросс-энтропия (cross-entropy loss)**
получается, если поставить перед функцией $likelihood$ минус, и задача
оптимизации меняется на противоположную: был поиск максимума&nbsp;&mdash; теперь
поиск минимума:
$$L(w)=logloss=-\displaystyle\sum_i^n(y_ilog(\widehat{P_i})+(1-y_i)log(1-\widehat{P_i}))\to min_w$$

----

**Матрица ошибок (confusion matrix)**&nbsp;&mdash; все возможные исходы
совпадения и несовпадения предсказания модели с действительностью. Используется
для расчёта других метрик.

## Метрики классификации ##

**Accuracy (достоверность/аккуратность)**&nbsp;&mdash; доля правильных ответов
модели среди всех ответов. Правильные ответы&nbsp;&mdash; это истинно
положительные (`True Positive`) и истинно отрицательные ответы (`True Negative`).

*Интерпретация:* как много (в долях) модель угадала ответов.
$$accuracy=\frac{TP+TN}{TP+TN+FP+FN}$$

**Precision (точность)** или **PPV (Positive Predictive Value)**&nbsp;&mdash;
доля объектов, которые действительно являются положительными, по отношению ко
всем объектам, названным моделью положительными.

*Интерпретация:* способность отделить класс **`1`** от класса **`0`**. Чем
больше `precision`, тем меньше ложных попаданий. То есть чем ближе `precision` к
**`1`**, тем меньше вероятность модели допустить ошибку **I рода**.
$$precision=\frac{TP}{TP+FP}$$
`Precision` нужен в задачах, где требуется минимум ложных срабатываний. Чем
выше &laquo;цена&raquo; ложноположительного результата, тем выше должен быть
`precision`.

**Recall (полнота)** или **TPR (True Positive Rate)**&nbsp;&mdash; доля
объектов, названных классификатором положительными, по отношению ко всем
объектам положительного класса.

*Интерпретация:* способность модели обнаруживать класс **`1`** вообще, то есть
охват класса **`1`**. `Recall` зависит от количества ложноотрицательных
срабатываний. То есть чем ближе `recall` к **`1`**, тем меньше вероятность
модели допустить ошибку **II рода**.
$$recall=\frac{TP}{TP+FN}$$
`Recall` очень хорошо себя показывает в задачах, где важно найти как можно
больше объектов, принадлежащих к классу **`1`**.

**$F_{\beta}$ (F-мера)**&nbsp;&mdash; **взвешенное среднее гармоническое** между
`precision` и `recall`.
$$F_{\beta}=(1+\beta^2)\frac{precision \times recall}{(\beta^2 \times precision)+recall}$$
где

$\beta$&nbsp;&mdash; это вес `precision` в метрике: чем больше $\beta$, тем
больше вклад.

Метрика $F_1$ (когда $\beta=1$) равна своему максимуму (**`1`**), если и
`precision`, и `recall` равны **`1`** (то есть когда отсутствуют как
ложноположительные, так и ложноотрицательные срабатывания). Но если хотя бы одна
из метрик будет близка к **`0`**, то и будет близка к **`0`**.

----
